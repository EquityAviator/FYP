# ğŸ•·ï¸ Full Website Recursive Crawler - Complete Guide

## ğŸ¯ **What It Does**

The **Full Website Crawl** feature recursively follows **ALL links** on a website to discover **EVERY single page**. Unlike the previous methods that only scanned one page, this crawler:

- âœ… **Starts from any page** (homepage, category page, etc.)
- âœ… **Visits every discovered page**
- âœ… **Follows links from each page**
- âœ… **Continues until ALL pages are found**
- âœ… **No page left behind!**

---

## ğŸš€ **How It Works**

### **Algorithm:**

```
1. Start with seed URL (current page)
2. Visit the page
3. Discover all links on that page
4. Add new links to queue
5. Visit next page in queue
6. Repeat until queue is empty
7. Result: ALL pages on the website discovered!
```

### **Smart Features:**

- âœ… **URL Normalization**: Removes duplicates (handles trailing slashes, fragments, query params)
- âœ… **Visited Tracking**: Never visits the same page twice
- âœ… **Smart Filtering**: Skips API endpoints, images, PDFs, admin pages
- âœ… **Rate Limiting**: 2-second delay between pages (respectful crawling)
- âœ… **Progress Tracking**: Shows discovered, visited, and queue counts in real-time
- âœ… **Safety Limit**: Maximum 10,000 pages (prevents infinite loops)

---

## ğŸ“‹ **How to Use**

### **Step 1: Navigate to Website**

1. Open the website you want to crawl (e.g., `https://www.daraz.pk/`)
2. You can start from:
   - Homepage
   - Category page
   - Any page on the site

### **Step 2: Start Full Crawl**

1. Open the extension popup
2. Click **"Batch Process (Auto Crawl)"**
3. You'll see 3 options:
   - **Quick Scan**: Current page only
   - **Deep Scan**: Current page with scrolling
   - **ğŸ•·ï¸ Full Website Crawl**: Recursively crawl ENTIRE website â† **Choose this!**

### **Step 3: Confirm and Start**

1. Click **"ğŸ•·ï¸ Full Website Crawl"**
2. Review the confirmation modal
3. Click **"Start Full Crawl"**

### **Step 4: Monitor Progress**

You'll see a progress card showing:
- **Discovered**: Total unique pages found
- **Visited**: Pages already crawled
- **In Queue**: Pages waiting to be visited
- **Current URL**: Page being crawled right now

### **Step 5: Wait for Completion**

- **Small sites** (100-500 pages): 5-15 minutes
- **Medium sites** (500-2000 pages): 15-30 minutes
- **Large sites** (2000+ pages): 30-60 minutes

**âš ï¸ Important**: Keep the extension popup open during crawling!

### **Step 6: Process Results**

When complete, you'll see:
- Total pages discovered
- Sample URLs
- Option to process all pages for dark pattern analysis

---

## ğŸ“Š **Expected Results**

### **For Daraz.pk:**

| Method | Pages Found | Time |
|--------|-------------|------|
| Quick Scan | 219 | 5s |
| Deep Scan | 500-2000 | 30-60s |
| **Full Website Crawl** | **5000-15000+** | **20-40 min** |

### **What Gets Discovered:**

- âœ… All product pages
- âœ… All category pages
- âœ… All subcategory pages
- âœ… All static pages (About, Contact, etc.)
- âœ… All search result pages
- âœ… All user pages (if accessible)
- âœ… Everything linked from anywhere on the site

---

## ğŸ”§ **Technical Details**

### **URL Normalization:**

The crawler normalizes URLs to avoid duplicates:
- Removes fragments (`#section`)
- Removes trailing slashes (`/page/` â†’ `/page`)
- Sorts query parameters
- Handles relative/absolute URLs

**Example:**
```
https://daraz.pk/product/123/
https://daraz.pk/product/123
https://daraz.pk/product/123?ref=home
```
All treated as the same URL!

### **Smart Filtering:**

**Excluded URLs:**
- âŒ API endpoints (`/api/`, `/ajax/`, `/json/`)
- âŒ Admin pages (`/admin/`, `/private/`)
- âŒ Static assets (`.pdf`, `.jpg`, `.png`, etc.)
- âŒ JavaScript/Email links
- âŒ External links (different domain)

**Included URLs:**
- âœ… Product pages
- âœ… Category pages
- âœ… Content pages
- âœ… Search pages
- âœ… User pages
- âœ… Any internal page

### **Rate Limiting:**

- **2 seconds delay** between page visits
- Prevents overwhelming the server
- Respectful crawling behavior
- Can be adjusted if needed

### **Safety Features:**

- **Max 10,000 pages**: Prevents infinite loops
- **Visited tracking**: Never revisits pages
- **Error handling**: Continues even if some pages fail
- **Progress saving**: Can resume if interrupted (future feature)

---

## âš ï¸ **Important Notes**

### **Time Requirements:**

- **Full website crawl takes TIME** (10-60 minutes)
- Keep browser and extension open
- Don't close the extension popup
- Be patient!

### **API Costs:**

- Each discovered page will be analyzed (if you process them)
- **5000 pages Ã— $0.02 = $100** (example)
- Consider processing in batches
- Or use UI-TARS for cost-effective analysis

### **Resource Usage:**

- Opens/closes tabs automatically
- Uses browser memory
- May slow down browser slightly
- Close other tabs if needed

### **Limitations:**

1. **JavaScript-heavy sites**: Some links may be missed if loaded dynamically
2. **Login-required pages**: Won't access pages behind authentication
3. **Rate limiting**: Some sites may block too many requests
4. **Infinite loops**: Safety limit prevents, but very large sites may hit limit

---

## ğŸ¯ **Best Practices**

### **For Maximum Coverage:**

1. âœ… **Start from homepage**: Gets all main sections
2. âœ… **Let it run completely**: Don't interrupt
3. âœ… **Process in batches**: If you get 5000+ pages, process 500 at a time
4. âœ… **Review results**: Check if important pages were discovered

### **For Cost Efficiency:**

1. âœ… **Crawl first, analyze later**: Discover all pages first
2. âœ… **Filter before processing**: Focus on product/checkout pages
3. âœ… **Use UI-TARS**: For large-scale analysis (free after setup)
4. âœ… **Export URLs**: Save discovered URLs for later analysis

### **For Large Sites:**

1. âœ… **Start early**: Crawling takes time
2. âœ… **Monitor progress**: Check discovered count
3. âœ… **Stop if needed**: Can manually stop (close popup)
4. âœ… **Resume later**: Re-run on missed sections if needed

---

## ğŸ“ˆ **Comparison: All Methods**

| Feature | Quick Scan | Deep Scan | **Full Website Crawl** |
|---------|-----------|-----------|------------------------|
| **Pages Found** | 50-200 | 200-2000 | **5000-15000+** |
| **Time** | 5s | 30-60s | **20-60 min** |
| **Coverage** | 1 page | 1 page | **ENTIRE website** |
| **Use Case** | Quick test | Category page | **Complete research** |
| **Best For** | Small sites | Medium sites | **Large e-commerce** |

---

## ğŸš¨ **Troubleshooting**

### **Crawler Stops Early:**

**Possible causes:**
- Site blocks too many requests
- JavaScript errors on some pages
- Network issues

**Solution:**
- Check browser console for errors
- Try starting from different page
- Reduce delay (if you modify code)

### **Too Many Pages Found:**

**If you get 10,000+ pages:**
- This is normal for large sites
- Process in batches (500-1000 at a time)
- Focus on product pages for dark patterns

### **Crawler Too Slow:**

**Normal behavior:**
- 2 seconds per page is intentional
- Prevents server overload
- Respectful crawling

**If needed:**
- Can reduce delay (modify code)
- But may get blocked by server

### **Missing Pages:**

**Some pages may be missed if:**
- Behind login/authentication
- Loaded via complex JavaScript
- Not linked from anywhere

**Solution:**
- Manually add important URLs
- Use "Batch Process (Manual URLs)"

---

## âœ… **Summary**

### **What You Get:**

- âœ… **Complete website map**: Every page discovered
- âœ… **No duplicates**: Smart URL normalization
- âœ… **Real-time progress**: See what's happening
- âœ… **Ready for analysis**: All URLs ready to process

### **Perfect For:**

- âœ… **Research projects**: Complete dataset collection
- âœ… **Large e-commerce sites**: Daraz, OLX, etc.
- âœ… **Comprehensive analysis**: Nothing left behind
- âœ… **FYP projects**: Complete website coverage

---

## ğŸ‰ **Ready to Use!**

The Full Website Crawler is now available in your extension. Just:

1. Go to any website
2. Click "Batch Process (Auto Crawl)"
3. Choose "ğŸ•·ï¸ Full Website Crawl"
4. Wait for complete discovery
5. Process all pages for dark pattern analysis

**No page will be left behind! ğŸš€**


